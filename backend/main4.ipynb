{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (0.5.4)\n",
      "Requirement already satisfied: llama-index in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (0.10.38)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (0.7.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (0.6.4)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (2.19.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (2.129.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-generativeai) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.38 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.10.38.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.10)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index) (0.1.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core->google-generativeai) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.7.2)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.30.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (2024.5.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-ng in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.6.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.4.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (9.0.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (3.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (8.3.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index) (1.12.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from pydantic->google-generativeai) (2.18.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai) (1.64.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai) (1.62.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama-index) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.38->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.4.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.21.2)\n",
      "Requirement already satisfied: ply in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from jsonpath-ng->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.38->llama-index) (2021.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (3.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.2.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\piyush\\anaconda3\\lib\\site-packages (from jinja2->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\piyush\\appdata\\roaming\\python\\python39\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama-index) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv google-generativeai llama-index tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "# Load data from a text file\n",
    "def load_data_from_txt(file_path, encodings=('utf-8', 'cp1252')):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "            return [Document(text=content)]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise Exception(\"Unable to decode the file using specified encodings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tokens\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_response(response, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(response.response))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data.txt' is your file\n",
    "file_path = \"cleaned_text_output.txt\"\n",
    "documents = load_data_from_txt(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb795fe06724411eaa9a3344cde8a25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\embeddings\\utils.py:59\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     58\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding()\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\embeddings\\openai\\utils.py:103\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the index\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\indices\\base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     transformations,\n\u001b[0;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    143\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    146\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    147\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    148\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    149\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    150\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m    151\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    153\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:71\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43membed_model_from_settings_or_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\settings.py:274\u001b[0m, in \u001b[0;36membed_model_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39membed_model\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\settings.py:67\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\llama_index\\core\\embeddings\\utils.py:66\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using embed_model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit our documentation for more embedding options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.html#modules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the retriever and postprocessor\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost calculation function\n",
    "def openai_api_calculate_cost(usage, model=\"gpt-3.5-turbo-16k\"):\n",
    "    pricing = {\n",
    "        'gpt-3.5-turbo-4k': {\n",
    "            'prompt': 0.0015,\n",
    "            'completion': 0.002,\n",
    "        },\n",
    "        'gpt-3.5-turbo-16k': {\n",
    "            'prompt': 0.003,\n",
    "            'completion': 0.004,\n",
    "        },\n",
    "        'gpt-4-8k': {\n",
    "            'prompt': 0.03,\n",
    "            'completion': 0.06,\n",
    "        },\n",
    "        'gpt-4-32k': {\n",
    "            'prompt': 0.06,\n",
    "            'completion': 0.12,\n",
    "        },\n",
    "        'text-embedding-ada-002-v2': {\n",
    "            'prompt': 0.0001,\n",
    "            'completion': 0.0001,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model_pricing = pricing[model]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "\n",
    "    prompt_cost = usage['prompt_tokens'] * model_pricing['prompt'] / 1000\n",
    "    completion_cost = usage['completion_tokens'] * model_pricing['completion'] / 1000\n",
    "\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    print(f\"\\nTokens used:  {usage['prompt_tokens']:,} prompt + {usage['completion_tokens']:,} completion = {usage['total_tokens']:,} tokens\")\n",
    "    print(f\"Total cost for {model}: ${total_cost:.4f}\\n\")\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Gunghat refers to a case of medical deception that was\n",
      "discussed in a blog post by Dr. Birbal.\n",
      "______________________________________________________________________\n",
      "Source Node 1/1\n",
      "Node ID: cc1666b1-c43b-4ec3-a224-2b010d314c64\n",
      "Similarity: 0.7701762671276723\n",
      "Text: Birbal Blog Internet of Things (IoT) Dr. Birbal, who is a modern\n",
      "day, seasoned and highly respected health claims Investigator Dr.\n",
      "Birbal Latest Blog Behind the Ghunghat: A Case of Medical Deception By\n",
      "Riyaz Lakhani In Dr. Birbal Posted March 28, 2023 Behind the Ghunghat:\n",
      "A Case of Medical Deception 0 Read More The Hospital Conspiracy\n",
      "Unveiled: ...\n"
     ]
    }
   ],
   "source": [
    "# Perform a query\n",
    "query = \"Gunghat?\"\n",
    "# response = query_engine.query(\"What is Video MER?\")\n",
    "# response = query_engine.query(\"What is TELE MER?\")\n",
    "# response = query_engine.query(\"How can we contact quicksolv?\")\n",
    "# response = query_engine.query(\"Does Video MER help me detect heartrate?\")\n",
    "# response = query_engine.query(\"Aadhar masking and IRDA?\")\n",
    "response = query_engine.query(query)\n",
    "# response = query_engine.query(\"where is Xangars located in Malaysia?\")\n",
    "\n",
    "# Print the response with source\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m encoding_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use the appropriate encoding for your model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m query_tokens \u001b[38;5;241m=\u001b[39m num_tokens_from_string(query, encoding_name)\n\u001b[1;32m----> 4\u001b[0m response_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnum_tokens_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens in query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mnum_tokens_from_string\u001b[1;34m(string, encoding_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_tokens_from_string\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m, encoding_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m      3\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(encoding_name)\n\u001b[1;32m----> 4\u001b[0m     num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num_tokens\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "encoding_name = \"cl100k_base\"  # Use the appropriate encoding for your model\n",
    "\n",
    "query_tokens = num_tokens_from_string(query, encoding_name)\n",
    "response_tokens = num_tokens_from_string(response, encoding_name)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Number of tokens in query: {query_tokens}\")\n",
    "print(f\"Number of tokens in response: {response_tokens}\")\n",
    "print(f\"Total number of tokens: {query_tokens + response_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Response' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens_from_messages(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompt tokens counted by num_tokens_from_messages().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m, in \u001b[0;36mnum_tokens_from_messages\u001b[1;34m(messages, model)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mnum_tokens_from_messages() is not implemented for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m     33\u001b[0m     num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokens_per_message\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Response' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(f\"{num_tokens_from_messages(response)} prompt tokens counted by num_tokens_from_messages().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnum_tokens_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: Gunghat?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m, in \u001b[0;36mnum_tokens_from_string\u001b[1;34m(string, encoding_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_tokens_from_string\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m, encoding_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m      3\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(encoding_name)\n\u001b[1;32m----> 4\u001b[0m     num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num_tokens\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "num_tokens = num_tokens_from_string(response)\n",
    "print(f\"Query: Gunghat?\")\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gunghat refers to a case of medical deception that was discussed in a blog post by Dr. Birbal.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     usage \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m     }\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken usage information not available in the response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'usage'"
     ]
    }
   ],
   "source": [
    "if hasattr(response, 'metadata'):\n",
    "    usage = {\n",
    "        'prompt_tokens': response.metadata['usage']['prompt_tokens'],\n",
    "        'completion_tokens': response.metadata['usage']['completion_tokens'],\n",
    "        'total_tokens': response.metadata['usage']['total_tokens'],\n",
    "    }\n",
    "else:\n",
    "    print(\"Token usage information not available in the response.\")\n",
    "    usage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m usage \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musage\u001b[49m\u001b[38;5;241m.\u001b[39mprompt_tokens,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mcompletion_tokens,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mtotal_tokens,\n\u001b[0;32m      5\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Response' object has no attribute 'usage'"
     ]
    }
   ],
   "source": [
    "usage = {\n",
    "    'prompt_tokens': response.usage.prompt_tokens,\n",
    "    'completion_tokens': response.usage.completion_tokens,\n",
    "    'total_tokens': response.usage.total_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'usage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate and print the cost\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Specify the model used\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m total_cost \u001b[38;5;241m=\u001b[39m openai_api_calculate_cost(\u001b[43musage\u001b[49m, model\u001b[38;5;241m=\u001b[39mmodel_used)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'usage' is not defined"
     ]
    }
   ],
   "source": [
    "if usage:\n",
    "    # Calculate and print the cost\n",
    "    model_used = \"gpt-3.5-turbo-16k\"  # Specify the model used\n",
    "    total_cost = openai_api_calculate_cost(usage, model=model_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
